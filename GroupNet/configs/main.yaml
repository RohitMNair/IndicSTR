img_size: 128
log_dir: /home/rohitn/D1/GrpNet_logs/new
name: FocalPosVisNet
# using SLURM Job Id which is obtained in the train script
version: ${oc.env:SLURM_JOB_ID}
# specify a sub-folder if continuing training; log path will be: log_dir/name/version
# version: 135399/${oc.env:SLURM_JOB_ID}_co
ckpt_path: null
restart_training: false # if training state needs a fresh start

model_load: # this won't be used if restart_training is false
  # _target_: model.FocalSTR.net.FocalSTR.load_from_checkpoint
  _target_: model.ViTSTR.net.ViTSTR.load_from_checkpoint
  checkpoint_path: ${ckpt_path}

defaults: # this wont be used if restart_training is true
  - model: FocalSTR # Options are FocalSTR, ViTSTR, GrpNet, FocalPosVisNet, FocalGrpNet
  - _self_

datamodule:
  _target_: data.module.LMDBDataModule
  train_dir: /home/rohitn/D1/HindiSynth/hindi_train_merged_lmdb
  val_dir: /home/rohitn/D1/HindiSynth/hindi_val_merged_lmdb
  test_dir: null
  batch_size: 64 
  num_workers: 8
  drop_last: false
  language: hindi
  remove_unseen: false

transforms:
  rotation: 60
  img_size: ${img_size}
  
csv_logger:
  _target_: lightning.pytorch.loggers.CSVLogger
  save_dir: ${log_dir} # Do not put "/" at the end
  name: ${name}
  version: ${version}

tensorboard_logger:
  _target_: lightning.pytorch.loggers.TensorBoardLogger
  save_dir:  ${log_dir}
  name: ${name}
  version: ${version}

model_checkpoint:
  _target_: lightning.pytorch.callbacks.ModelCheckpoint
  dirpath: ${log_dir}/${name}/${version}/checkpoints/      
  filename: "{epoch}-{val_loss:.2f}-{val_wrr2:.2f}"
  monitor: val_loss
  mode: min
  save_top_k: 3
  save_last: true

training:
  _target_: lightning.pytorch.Trainer
  accelerator: gpu # options are cpu or gpu
  strategy: ddp # only use when gpu is available
  devices: 2
  max_epochs: 10
  # max_steps: 1000
  check_val_every_n_epoch: 1
  gradient_clip_val: 10
  precision: 32-true
  log_every_n_steps: 100
  num_nodes: 1

hydra:
  run:
    dir: ${log_dir}/${name}/${version}